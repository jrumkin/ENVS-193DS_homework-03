---
title: "Homework 3"
author: "Jessi Rumkin"
date: "May 22, 2025"
format:
  html:
    toc: true # to display a table of contents
execute:
  message: false # make sure messages don't show up
  warning: false # make sure warnings don't show up
---

Link to GitHub repository: 
[https://github.com/jrumkin/ENVS-193DS_homework-03](https://github.com/jrumkin/ENVS-193DS_homework-03)

# Set Up

```{r}
#read in packages
library(tidyverse)
library(here)
library(gt)
library(janitor)
library(readxl)

#read in data for personal data project
mydata <- read.csv("C:/Users/jlrum/MyGit/ENVS-193DS_homework-03/data/mydata.csv")

```

# Problem 1: Personal Data

## a. Data summarizing

I will be comparing the mean productivity score between two main groups, when I was alone and when I was accompanied for the study sessions in order to determine if working with other people increases my productivity, which is calculated from time spent working and how focused I was. This can help me determine if there is a statistical or significant difference between groups, because I feel like I spend more time on a task in one sitting when I am with other people, so I think my mean productivity will be higher in the accompanied group.

## b. Visualization

```{r}
#summarizing personal data
mydata_summary <-  mydata |> #make an object called mydata_summary using mydata file
  group_by(accompanied) |> #group the sumary by company type
  summarise(mean_score = round(mean(final_productivity_score),digits = 1), #find mean
            sd = round(sd(final_productivity_score), digits = 1), #find standard deviation
            se = round((sd(final_productivity_score)/sqrt(length(final_productivity_score))),digits = 1), #find standard error
            ci_lower = round(mean_score - 1.96 * (sd(final_productivity_score)/ sqrt(length(final_productivity_score))),digits = 1), #find lower confidence interval at 95%
            ci_upper = round(mean_score + 1.96 * (sd(final_productivity_score) / sqrt(length(final_productivity_score))),digits = 1) #find upper confidence interval at 95%
  )


# exploring personal data distribution

ggplot(data = mydata,
       aes(sample = final_productivity_score)) +
  geom_qq() +
  geom_qq_line() +
  facet_wrap(~accompanied)

#almost normal data! Alone data is like not really that normal, there are some outliers so idk if this should count. 

```

```{r}

#creating visualizations for personal data, Figure 1.

mydata_means <- ggplot(data = mydata, #use ggplot to create a visualization called mydata_means with data from mydata
                       aes(x = accompanied,
                           y = final_productivity_score,
                           color = accompanied)) +
  geom_jitter(height = 0, #add geometry of jitter plot, 0 height jitter
              width = 0.2, #0.2 jitter width
              shape = 20) + #shape is open circles for the jitter points
  geom_errorbar(data = mydata_summary, #add a geometry of an error bar from data of mydata_summary
                aes(x = accompanied,
                    y = mean_score,
                    ymin = ci_lower,
                    ymax = ci_upper,
                    width = 0.15)) +
  geom_point(data = mydata_summary, #add geometry of a point from mydata_summary of the mean productivity score
             aes(x = accompanied,
                 y = mean_score),
             inherit.aes = FALSE, #the geom_point layer will not inherit aesthitcs settings
             color = "black", #point showing the mean is black
             size = 2) +
  labs(x = "Company Type", #labling the x and y axis and the title
       y = "Final Productivity Score",
       title = "Mean Productivity Score between \nAccompanied and Alone study sessions") +
  scale_color_manual(values = c("Accompanied" = "#377b42", #manual colors for groups 
                                "Alone" = "#0c78bd")) +
  theme(legend.position = "none") #removing the legend for redundancy

mydata_means #display the visualization


```

```{r}


# visualizing deadline type vs final productivity score
mydata_deadlinesummary <- mydata |> 
  group_by(deadline_type) |> 
  summarise(mean_score = mean(final_productivity_score))

mydata_deadline <- ggplot(data = mydata,
                          aes(x = deadline_type,
                              y = final_productivity_score,
                              color = deadline_type)) +
  geom_jitter(height = 0,
              width = 0.2,
              shape = 1) +
  geom_point(data = mydata_deadlinesummary,
             aes(x = deadline_type,
                 y = mean_score),
             inherit.aes = FALSE,
             color = "black",
             size = 2)


mydata_deadline

#visualizing productivity vs sleep

mydata_sleep <- ggplot(data = mydata, #creating a visualization with ggplot, obect name is mydata_sleep
       aes(x = sleep_decimal_hour, #sleep in decimal hours on the x axis
           y = final_productivity_score)) + #final productivity score on the y axis
  geom_point() + #add scatter plot geometry
  geom_smooth(method = lm, se = TRUE) #add a line of best fit, lm = linear model, se = standard error shading is shown

mydata_sleep #print the mydata_sleep plot

```

## c. Caption

**Figure 1. Average productivity scores are higher for accompanied study sessions compared to alone study sessions. ** The black point is the mean productivity score in each group, with error bars showing the confidence interval of the mean at a 95% confidence level, while open points represent individual data points. Colored by group type of accompanied (n = __) and alone (n = __)

## d. Table presentation

```{r}

#using gt package to make a table

summary_table <- gt(mydata_summary) |> # creating a summary table of final productivity score using the previously created mydata_summary
  cols_label( #manual labels for each column
    mean_score = "Mean Score",
    sd = "Standard Deviation",
    se = "Standard Error",
    ci_lower = "Confidence Interval, lower",
    ci_upper = "Confidence Interval, upper"
  ) |> 
  tab_header( #title that appears above the table
    title = "Productivity Score Calculation Summary by Study Session Type"
  )

summary_table #print the table

```


# Problem 2. Affective Visualization

## a. Describe what an affective visualization could look like for personal data.

For my personality and executive function type I tend to get more done when I have external cues or deadlines, so I'm not surprised that my data shows I may get more done when I am working with others. I want my affective visualization to focus on community and the achievement of actually getting all that work done. Something I wasn't expecting was that the plot is also a visualization of all my time and effort, so I also want the affective visualization to focus on accomplishment and not on comparison. 

## b. Create a sketch (on paper) of your idea

## c. Make a draft of your visualization

## d. Write an artist statement


# Problem 3. Statstical Critique

Paper Citation: Robuck, A. R., Hudak, C. A., Agvent, L., Emery, G., Ryan, P. G., Perold, V., Powers, K. D., Pedersen, J., Thompson, M. A., Suca, J. J., Moore, M. J., Harms, C. A., Bugoni, L., Shield, G., Glass, T., Wiley, D. N., & Lohmann, R. (2022). Birds of a feather eat plastic Together: High levels of plastic ingestion in great shearwater adults and juveniles across their annual migratory cycle. Frontiers in Marine Science, 8. <https://doi.org/10.3389/fmars.2021.719721>

DOI: [https://doi.org/10.3389/fmars.2021.719721](https://doi.org/10.3389/fmars.2021.719721)

## a. Revist and Summarize

These authors are using the Kruskill Wallis test to determine if there are differences between several different groups of bird region and age, and the variable being tested is plastic fragment size for both length and area. 

![figure 4](figure4.png)


## b. Visual clarity

The authors clearly label the axis and legend of this figure, but I wish the legend was more prominent or the figure included a title to explain the point of the figure. On first glance it looks like the take away is that as plastic fragments get longer, they get wider or have more surface area, but the actual take away is that certain regions have larger plastic fragments found in the birds from those regions. The summary statistic means are the main data points being displayed, and they also include 95% confidence interval error bars as well as underlying data. I like that the data is separated by color and shape, and while I understand why they excluded the upper 5% of data I would have like to seen it especially because the averages seem so much larger than were most of the data lies. I also noticed that figure 4b doesn't have feet/tops to the error bars, which isn't really a bad thing just something I noticed. 

## c. Aesthetic clarity

Figure 4 looks quite cluttered but I appreciate the effort that went into making it easier to look at. Each location has a different color and shape in the underlying data but because there is so much of it all the points just blur together into a brown grey color unless you zoom in a lot. I also don't like the choice to exclude the upper 5% of the data points because the means look so much larger than where most of the data is, so it appears like the excluded outliers are having a large effect on the means. I think the data to ink ratio is decent but could be improved because they try to display a lot of data but the formatting and text size of the axis are too large in comparison to the legend which actually tells the reader the point of the graph. 




